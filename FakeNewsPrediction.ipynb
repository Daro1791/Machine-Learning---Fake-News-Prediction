{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use machine learning to create a model to identify when an article might be fake news.\n",
    "# We will use logistic Regression.\n",
    "# The data comes from Kaggle page (https://www.kaggle.com/c/fake-news#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s import the libraries we are going to use.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:\n",
      "{'further', 'few', 'you', 'until', 'whom', 'what', \"needn't\", 'than', 'up', 'being', 'some', 'can', 'hers', 's', 'ourselves', 'because', 'off', 'down', 'of', 'which', 'were', 'isn', 'and', 'about', 'on', 're', 'itself', 'when', 'other', 'shouldn', 'here', 'their', 'too', \"haven't\", 'does', \"wasn't\", 'he', 'in', 'haven', 'didn', 'no', 'did', 'between', \"she's\", 'her', 'more', 'it', 'ma', 'yourselves', 'weren', 'me', 'if', \"didn't\", 'an', 'its', 'having', 'him', 'doing', 'both', \"wouldn't\", 'i', 'there', 'once', 'as', 'for', \"won't\", \"weren't\", 'just', \"hasn't\", 'a', \"couldn't\", 'only', \"aren't\", 'from', 'won', 'mightn', 'very', 'ours', \"mustn't\", 'my', 'over', 'after', 'd', 'any', 'm', \"doesn't\", 'yours', 'yourself', 'such', 'with', 'himself', 'out', 'during', 'they', 'why', 'or', 'll', 'all', 'now', \"you'd\", 'do', 'y', \"you'll\", 'been', 'under', 'she', \"should've\", 'not', 'again', 'these', 'have', 'those', 'against', \"shan't\", 'above', \"that'll\", 'o', 'shan', 'was', \"you've\", 'most', 'has', 'are', 'through', 'the', 'doesn', 'same', 'am', 'into', 'our', 'theirs', 'by', 'at', 'be', 'then', 'so', 've', \"hadn't\", \"mightn't\", 'herself', \"it's\", 'own', 'hadn', 'below', 'them', 'couldn', 'aren', \"you're\", 'your', 'before', 'how', 'each', 'his', 'mustn', \"shouldn't\", \"don't\", 'myself', 'wasn', 'themselves', 'while', 'where', 'nor', 'will', 'ain', 'needn', 'who', 'this', 'to', 'hasn', 'wouldn', 'but', \"isn't\", 'don', 'had', 'is', 'we', 'that', 't', 'should'}\n",
      "Total number of words:  179\n"
     ]
    }
   ],
   "source": [
    "# You can view the list of included stop words in NLTK using the code below:\n",
    "stops = set(stopwords.words('english'))\n",
    "print('Words:')\n",
    "print(stops)\n",
    "print('Total number of words: ', len(stops))\n",
    "# You can do that for different languages, so you can configure for the language you need.\n",
    "# stops = set(stopwords.words('german'))\n",
    "# stops = set(stopwords.words('indonesia'))\n",
    "# stops = set(stopwords.words('portuguese'))\n",
    "# stops = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset using pandas\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "Head\n",
      "   id                                              title              author  \\\n",
      "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
      "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
      "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
      "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
      "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
      "\n",
      "                                                text  label  \n",
      "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
      "1  Ever get the feeling your life circles the rou...      0  \n",
      "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
      "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
      "4  Print \\nAn Iranian woman has been sentenced to...      1  \n",
      "Tail\n",
      "          id                                              title  \\\n",
      "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
      "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
      "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
      "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
      "20799  20799                          What Keeps the F-35 Alive   \n",
      "\n",
      "                                          author  \\\n",
      "20795                              Jerome Hudson   \n",
      "20796                           Benjamin Hoffman   \n",
      "20797  Michael J. de la Merced and Rachel Abrams   \n",
      "20798                                Alex Ansary   \n",
      "20799                              David Swanson   \n",
      "\n",
      "                                                    text  label  \n",
      "20795  Rapper T. I. unloaded on black celebrities who...      0  \n",
      "20796  When the Green Bay Packers lost to the Washing...      0  \n",
      "20797  The Macy’s of today grew from the union of sev...      0  \n",
      "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1  \n",
      "20799    David Swanson is an author, activist, journa...      1  \n",
      "Shape: (20800, 5)\n"
     ]
    }
   ],
   "source": [
    "# Summarize the Dataset\n",
    "print('df')\n",
    "print('Head')\n",
    "print(df.head())\n",
    "print('Tail')\n",
    "print(df.tail())\n",
    "print('Shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data\n",
      "id           0\n",
      "title      558\n",
      "author    1957\n",
      "text        39\n",
      "label        0\n",
      "dtype: int64\n",
      "Missing data in percentage\n",
      "id        0.00\n",
      "title     2.68\n",
      "author    9.41\n",
      "text      0.19\n",
      "label     0.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# We see if there is missing data\n",
    "print(\"Missing data\")\n",
    "print(df.isnull().sum())\n",
    "# In percentage\n",
    "print(\"Missing data in percentage\")\n",
    "print(round(df.isnull().sum()/df.shape[0]*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data\n",
      "id        0\n",
      "title     0\n",
      "author    0\n",
      "text      0\n",
      "label     0\n",
      "dtype: int64\n",
      "Missing data in percentage\n",
      "id        0.0\n",
      "title     0.0\n",
      "author    0.0\n",
      "text      0.0\n",
      "label     0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values with empty string\n",
    "df=df.fillna('')\n",
    "print(\"Missing data\")\n",
    "print(df.isnull().sum())\n",
    "# In percentage\n",
    "print(\"Missing data in percentage\")\n",
    "print(round(df.isnull().sum()/df.shape[0]*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                              title  \\\n",
      "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
      "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
      "2          2                  Why the Truth Might Get You Fired   \n",
      "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
      "4          4  Iranian woman jailed for fictional unpublished...   \n",
      "...      ...                                                ...   \n",
      "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
      "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
      "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
      "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
      "20799  20799                          What Keeps the F-35 Alive   \n",
      "\n",
      "                                          author  \\\n",
      "0                                  Darrell Lucus   \n",
      "1                                Daniel J. Flynn   \n",
      "2                             Consortiumnews.com   \n",
      "3                                Jessica Purkiss   \n",
      "4                                 Howard Portnoy   \n",
      "...                                          ...   \n",
      "20795                              Jerome Hudson   \n",
      "20796                           Benjamin Hoffman   \n",
      "20797  Michael J. de la Merced and Rachel Abrams   \n",
      "20798                                Alex Ansary   \n",
      "20799                              David Swanson   \n",
      "\n",
      "                                                    text  label  \\\n",
      "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
      "1      Ever get the feeling your life circles the rou...      0   \n",
      "2      Why the Truth Might Get You Fired October 29, ...      1   \n",
      "3      Videos 15 Civilians Killed In Single US Airstr...      1   \n",
      "4      Print \\nAn Iranian woman has been sentenced to...      1   \n",
      "...                                                  ...    ...   \n",
      "20795  Rapper T. I. unloaded on black celebrities who...      0   \n",
      "20796  When the Green Bay Packers lost to the Washing...      0   \n",
      "20797  The Macy’s of today grew from the union of sev...      0   \n",
      "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1   \n",
      "20799    David Swanson is an author, activist, journa...      1   \n",
      "\n",
      "                                                   total  \n",
      "0      Darrell Lucus House Dem Aide: We Didn’t Even S...  \n",
      "1      Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...  \n",
      "2      Consortiumnews.com Why the Truth Might Get You...  \n",
      "3      Jessica Purkiss 15 Civilians Killed In Single ...  \n",
      "4      Howard Portnoy Iranian woman jailed for fictio...  \n",
      "...                                                  ...  \n",
      "20795  Jerome Hudson Rapper T.I.: Trump a ’Poster Chi...  \n",
      "20796  Benjamin Hoffman N.F.L. Playoffs: Schedule, Ma...  \n",
      "20797  Michael J. de la Merced and Rachel Abrams Macy...  \n",
      "20798  Alex Ansary NATO, Russia To Hold Parallel Exer...  \n",
      "20799            David Swanson What Keeps the F-35 Alive  \n",
      "\n",
      "[20800 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# We will be only using title and author name for prediction (we can also use the text column but it will take much time later).\n",
    "df['total'] = df['author'] + ' ' + df['title']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing a word to its root word\n",
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying stemming to the \"total\" column of the dataframe (this will take some time)\n",
    "df['total'] = df['total'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800,)\n",
      "(20800,)\n"
     ]
    }
   ],
   "source": [
    "# Separating the data and label\n",
    "X = df['total'].values\n",
    "Y = df['label'].values\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the textual data to numerical data\n",
    "vectorizer = TfidfVectorizer() # Term Frequency - Inverse Document Frequency.\n",
    "vectorizer.fit(X)\n",
    "\n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 17128)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15686)\t0.28485063562728646\n",
      "  (0, 13473)\t0.2565896679337957\n",
      "  (0, 8909)\t0.3635963806326075\n",
      "  (0, 8630)\t0.29212514087043684\n",
      "  (0, 7692)\t0.24785219520671603\n",
      "  (0, 7005)\t0.21874169089359144\n",
      "  (0, 4973)\t0.233316966909351\n",
      "  (0, 3792)\t0.2705332480845492\n",
      "  (0, 3600)\t0.3598939188262559\n",
      "  (0, 2959)\t0.2468450128533713\n",
      "  (0, 2483)\t0.3676519686797209\n",
      "  (0, 267)\t0.27010124977708766\n",
      "  (1, 16799)\t0.30071745655510157\n",
      "  (1, 6816)\t0.1904660198296849\n",
      "  (1, 5503)\t0.7143299355715573\n",
      "  (1, 3568)\t0.26373768806048464\n",
      "  (1, 2813)\t0.19094574062359204\n",
      "  (1, 2223)\t0.3827320386859759\n",
      "  (1, 1894)\t0.15521974226349364\n",
      "  (1, 1497)\t0.2939891562094648\n",
      "  (2, 15611)\t0.41544962664721613\n",
      "  (2, 9620)\t0.49351492943649944\n",
      "  (2, 5968)\t0.3474613386728292\n",
      "  (2, 5389)\t0.3866530551182615\n",
      "  (2, 3103)\t0.46097489583229645\n",
      "  :\t:\n",
      "  (20797, 13122)\t0.2482526352197606\n",
      "  (20797, 12344)\t0.27263457663336677\n",
      "  (20797, 12138)\t0.24778257724396507\n",
      "  (20797, 10306)\t0.08038079000566466\n",
      "  (20797, 9588)\t0.174553480255222\n",
      "  (20797, 9518)\t0.2954204003420313\n",
      "  (20797, 8988)\t0.36160868928090795\n",
      "  (20797, 8364)\t0.22322585870464118\n",
      "  (20797, 7042)\t0.21799048897828688\n",
      "  (20797, 3643)\t0.21155500613623743\n",
      "  (20797, 1287)\t0.33538056804139865\n",
      "  (20797, 699)\t0.30685846079762347\n",
      "  (20797, 43)\t0.29710241860700626\n",
      "  (20798, 13046)\t0.22363267488270608\n",
      "  (20798, 11052)\t0.4460515589182236\n",
      "  (20798, 10177)\t0.3192496370187028\n",
      "  (20798, 6889)\t0.32496285694299426\n",
      "  (20798, 5032)\t0.4083701450239529\n",
      "  (20798, 1125)\t0.4460515589182236\n",
      "  (20798, 588)\t0.3112141524638974\n",
      "  (20798, 350)\t0.28446937819072576\n",
      "  (20799, 14852)\t0.5677577267055112\n",
      "  (20799, 8036)\t0.45983893273780013\n",
      "  (20799, 3623)\t0.37927626273066584\n",
      "  (20799, 377)\t0.5677577267055112\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split out df into training data for preparing the models and testing data that we will use for testing them.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Train set 0.9866\n",
      "Test set 0.9791\n",
      "Confusion Matrix:\n",
      "[[2004   73]\n",
      " [  14 2069]]\n",
      "Accuracy: 0.9791\n",
      "Precision: 0.9659\n",
      "Recall: 0.9933\n",
      "F1 Score: 0.9794\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98      2077\n",
      "           1       0.97      0.99      0.98      2083\n",
      "\n",
      "    accuracy                           0.98      4160\n",
      "   macro avg       0.98      0.98      0.98      4160\n",
      "weighted avg       0.98      0.98      0.98      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression and metrics\n",
    "Logistic = LogisticRegression()\n",
    "Logistic.fit(X_train, Y_train)\n",
    "Y_pred = Logistic.predict(X_test)\n",
    "print('Logistic Regression:')\n",
    "print('Train set', round(Logistic.score(X_train, Y_train), 4))\n",
    "print('Test set', round(Logistic.score(X_test, Y_test), 4))\n",
    "\n",
    "# Metrics\n",
    "matrix = confusion_matrix(Y_test, Y_pred) \n",
    "print('Confusion Matrix:') # Confusion Matrix [TN FP] ; [FN, TP]\n",
    "print(matrix)\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_pred) # Accuracy: TN+TP / TN+TP+FN+FP\n",
    "print('Accuracy:', round(accuracy, 4))\n",
    "\n",
    "precision = precision_score(Y_test, Y_pred) # Precision: TP / TP+FP\n",
    "print('Precision:', round(precision,4))\n",
    "\n",
    "recall = recall_score(Y_test, Y_pred) # Recall: TP / TP+FN\n",
    "print('Recall:', round(recall, 4))\n",
    "\n",
    "f1 = f1_score(Y_test, Y_pred) # F1 Score (precision+recall): 2TP / 2TP+FN+FP\n",
    "print('F1 Score:', round(f1, 4))\n",
    "\n",
    "classificationRep = classification_report(Y_test, Y_pred) # Classification Report\n",
    "print('Classification Report:')\n",
    "print(classificationRep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
